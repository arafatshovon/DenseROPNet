{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11568613,"sourceType":"datasetVersion","datasetId":7253123},{"sourceId":12667635,"sourceType":"datasetVersion","datasetId":8005140},{"sourceId":12738031,"sourceType":"datasetVersion","datasetId":8005978}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# \"ResNet50-DualPoolNet: A Dual Pooling ResNet50 Model with Custom Dense Layers and Regularization for Robust Classification","metadata":{}},{"cell_type":"markdown","source":"\n## 1. Data loading and Preprocessing as Batch \n","metadata":{}},{"cell_type":"code","source":"##BATCH LOADED AND PREPROCESSED\nimport os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Define dataset directory and class names\nDATA_DIR = \"/kaggle/input/rop-2-dataset\"  # placeholder path\nclasses = [\"Normal\", \"Stage1\", \"Stage2\",\"Stage3\"]\nclass_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n\n# Collect filepaths and labels\nimage_paths = []\nlabels = []\nfor cls in classes:\n    cls_dir = os.path.join(DATA_DIR, cls)\n    for filename in os.listdir(cls_dir):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # image file\n            image_paths.append(os.path.join(cls_dir, filename))\n            labels.append(class_to_idx[cls])\n\nprint(f\"Found {len(image_paths)} images in total across {len(classes)} classes.\")\n# Example: print count per class for verification\ncounts = Counter(labels)\nfor cls_idx, count in counts.items():\n    print(f\"Class {classes[cls_idx]}: {count} images\")\n\n# Batch generator to load and preprocess images on-the-fly\ndef image_generator(image_paths, labels, batch_size=32):\n    batch_images = []\n    batch_labels = []\n    for i, path in enumerate(image_paths):\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (299, 299))  # UNet typically works with sizes that are powers of 2\n\n        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n        l, a, b = cv2.split(lab)  # Split into L, A, B channels\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Initialize CLAHE\n        cl = clahe.apply(l)  # Apply CLAHE to L channel\n        limg = cv2.merge((cl, a, b))  # Merge back the channels\n        img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n\n        batch_images.append(img / 299.0)  # Normalize to 0-1\n        batch_labels.append(labels[i])\n\n        if len(batch_images) == batch_size:\n            yield np.array(batch_images), np.array(batch_labels)\n            batch_images, batch_labels = [], []  # Reset the batch after yielding\n\n    # Yield remaining data if it's not a full batch\n    if batch_images:\n        yield np.array(batch_images), np.array(batch_labels)\n\n# Create the train-test split\ny = np.array(labels, dtype=np.int32)\nX_train, X_val, y_train, y_val = train_test_split(\n    image_paths, y, test_size=0.2, stratify=y, random_state=42)\n\n\nprint(\"Train set:\", len(X_train), \"Validation set:\", len(X_val))\n\n# Verify class distribution in each split\ndef print_class_dist(y_arr, name):\n    counts = Counter(y_arr)\n    dist = {classes[c]: counts[c] for c in sorted(counts.keys())}\n    print(f\"{name} class distribution:\", dist)\n\nprint_class_dist(y_train, \"Training\")\nprint_class_dist(y_val, \"Validation\")\n\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T16:45:10.720767Z","iopub.execute_input":"2025-08-17T16:45:10.721307Z","iopub.status.idle":"2025-08-17T16:45:12.249205Z","shell.execute_reply.started":"2025-08-17T16:45:10.721284Z","shell.execute_reply":"2025-08-17T16:45:12.248449Z"}},"outputs":[{"name":"stdout","text":"Found 11920 images in total across 4 classes.\nClass Normal: 2980 images\nClass Stage1: 2980 images\nClass Stage2: 2980 images\nClass Stage3: 2980 images\nTrain set: 9536 Validation set: 2384\nTraining class distribution: {'Normal': 2384, 'Stage1': 2384, 'Stage2': 2384, 'Stage3': 2384}\nValidation class distribution: {'Normal': 596, 'Stage1': 596, 'Stage2': 596, 'Stage3': 596}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"\n ## 2. Model Define \n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.layers import (\n    Input, GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, Dropout,\n    BatchNormalization, Reshape, Multiply, Add, Concatenate, Conv2D,\n    Activation, MaxPooling2D\n)\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\n\ndef squeeze_excitation_block(input_tensor, ratio=16, name_prefix=\"se\"):\n    \"\"\"\n    Squeeze and Excitation block\n    Args:\n        input_tensor: input feature map\n        ratio: reduction ratio for the bottleneck\n        name_prefix: prefix for layer names\n    \"\"\"\n    channels = tf.keras.backend.int_shape(input_tensor)[-1]\n\n    # Squeeze: Global Average Pooling\n    se = GlobalAveragePooling2D(name=f\"{name_prefix}_gap\")(input_tensor)\n\n    # Excitation: FC -> ReLU -> FC -> Sigmoid\n    se = Dense(\n        units=channels // ratio,\n        activation=\"relu\",\n        name=f\"{name_prefix}_dense1\"\n    )(se)\n\n    se = Dense(\n        units=channels,\n        activation=\"sigmoid\",\n        name=f\"{name_prefix}_dense2\"\n    )(se)\n\n    # Reshape to match input dimensions\n    se = Reshape((1, 1, channels), name=f\"{name_prefix}_reshape\")(se)\n\n    # Scale the input tensor\n    scaled = Multiply(name=f\"{name_prefix}_multiply\")([input_tensor, se])\n\n    return scaled\n\nclass SEDenseNet121(tf.keras.Model):\n    \"\"\"\n    DenseNet121 with SE blocks after convolutional blocks\n    \"\"\"\n    def __init__(self, include_top=False, weights='imagenet', input_shape=(299, 299, 3)):\n        super(SEDenseNet121, self).__init__()\n\n        # Create base DenseNet121\n        self.base_model = DenseNet121(\n            include_top=include_top,\n            weights=weights,\n            input_shape=input_shape\n        )\n\n        # Make all layers trainable\n        for layer in self.base_model.layers:\n            layer.trainable = True\n\n        # We'll add SE blocks at key points in the forward pass\n\n    def call(self, inputs):\n        x = inputs\n\n        # Process through DenseNet layers with SE blocks inserted\n        # Initial conv block\n        x = self.base_model.layers[1](x)  # ZeroPadding2D\n        x = self.base_model.layers[2](x)  # Conv2D\n        x = self.base_model.layers[3](x)  # BatchNormalization\n        x = self.base_model.layers[4](x)  # Activation\n        x = squeeze_excitation_block(x, name_prefix=\"se_initial\")\n        x = self.base_model.layers[5](x)  # ZeroPadding2D\n        x = self.base_model.layers[6](x)  # MaxPooling2D\n\n        # Dense Block 1\n        for i in range(7, 19):  # Dense block 1 layers\n            x = self.base_model.layers[i](x)\n        x = squeeze_excitation_block(x, name_prefix=\"se_block1\")\n\n        # Transition Layer 1\n        for i in range(19, 22):  # Transition layer 1\n            x = self.base_model.layers[i](x)\n\n        # Dense Block 2\n        for i in range(22, 46):  # Dense block 2 layers\n            x = self.base_model.layers[i](x)\n        x = squeeze_excitation_block(x, name_prefix=\"se_block2\")\n\n        # Transition Layer 2\n        for i in range(46, 49):  # Transition layer 2\n            x = self.base_model.layers[i](x)\n\n        # Dense Block 3\n        for i in range(49, 121):  # Dense block 3 layers\n            x = self.base_model.layers[i](x)\n        x = squeeze_excitation_block(x, name_prefix=\"se_block3\")\n\n        # Transition Layer 3\n        for i in range(121, 124):  # Transition layer 3\n            x = self.base_model.layers[i](x)\n\n        # Dense Block 4\n        for i in range(124, len(self.base_model.layers)):  # Dense block 4 layers\n            x = self.base_model.layers[i](x)\n        x = squeeze_excitation_block(x, name_prefix=\"se_block4\")\n\n        return x\n\n# Simpler approach: Add SE blocks at strategic points using functional API\ndef create_densenet_with_se_functional():\n    \"\"\"\n    Create DenseNet121 with SE blocks using functional API approach\n    \"\"\"\n    inputs = Input(shape=(299, 299, 3), name=\"input_image\")\n\n    # Create base DenseNet121\n    base_model = DenseNet121(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs\n    )\n\n    # Make all layers trainable\n    for layer in base_model.layers:\n        layer.trainable = True\n\n    # Get intermediate outputs at key points for SE block insertion\n    # We'll extract features from different dense blocks\n\n    # Get the base features\n    x = base_model.output\n\n    # Add SE block to the final features\n    x = squeeze_excitation_block(x, name_prefix=\"se_final\")\n\n    # You can also add SE blocks to intermediate layers if needed\n    # For now, let's add one comprehensive SE block at the end\n\n    return x\n\n# 1) Model Input and Backbone with SE blocks\ninputs = Input(shape=(299, 299, 3), name=\"input_image\")\n\n# Method 1: Use the functional approach (simpler and more stable)\nbase_model = DenseNet121(\n    include_top=False,\n    weights='imagenet',\n    input_tensor=inputs\n)\n\n# Freeze all DenseNet layers initially (as in original code)\nfor layer in base_model.layers:\n    layer.trainable = True\n\n# Extract features and add SE blocks at strategic points\nfeatures = base_model.output\n\n# Add SE blocks after the main feature extraction\nfeatures = squeeze_excitation_block(features, ratio=16, name_prefix=\"se_backbone_1\")\n\n# Add another SE block for enhanced feature refinement\nfeatures = squeeze_excitation_block(features, ratio=16, name_prefix=\"se_backbone_2\")\n\n# 2) Residual Attention Network Block (RANB) - keeping original implementation\n# Channel Attention (Squeeze-and-Excitation Block) - this is your original RANB\nchannel_descriptor = GlobalAveragePooling2D(name=\"att_gap\")(features)\n\n# Bottleneck MLP: reduce then restore channel dimension\nchannels = tf.keras.backend.int_shape(features)[-1]\nse = Dense(\n    units=channels // 16,\n    activation=\"relu\",\n    name=\"att_dense1\"\n)(channel_descriptor)\nse = Dense(\n    units=channels,\n    activation=\"sigmoid\",\n    name=\"att_dense2\"\n)(se)\n\n# Excitation: scale feature maps by channel weights\nse = Reshape((1, 1, channels), name=\"att_reshape\")(se)\nscaled = Multiply(name=\"att_scale\")([features, se])\n\n# Residual connection: add back to original features\nattended = Add(name=\"att_residual\")([features, scaled])\n\n# Spatial Attention: Apply 3x3 convolution for spatial attention\nspatial_attention = Conv2D(\n    filters=1,\n    kernel_size=3,\n    padding=\"same\",\n    activation=\"sigmoid\",\n    name=\"spatial_attention\"\n)(attended)\n\n# Scale the features by spatial attention\nattended = Multiply(name=\"att_spatial_scale\")([attended, spatial_attention])\n\n# 3) Dual Global Pooling - keeping original implementation\ngap = GlobalAveragePooling2D(name=\"dual_gap\")(attended)\ngmp = GlobalMaxPooling2D(name=\"dual_gmp\")(attended)\npooled = Concatenate(name=\"dual_concat\")([gap, gmp])\npooled = BatchNormalization(name=\"dual_bn\")(pooled)\n\n# 4) Classification Head - keeping original implementation\nx = Dense(\n    units=512,\n    activation=\"swish\",\n    kernel_regularizer=regularizers.l2(1e-4),\n    name=\"fc1\"\n)(pooled)\nx = Dropout(0.5, name=\"drop1\")(x)\n\nx = Dense(\n    units=256,\n    activation=\"swish\",\n    kernel_regularizer=regularizers.l2(1e-4),\n    name=\"fc2\"\n)(x)\nx = Dropout(0.5, name=\"drop2\")(x)\n\noutputs = Dense(\n    units=5,\n    activation=\"softmax\",\n    name=\"predictions\"\n)(x)\n\n# 5) Assemble & Compile - keeping original implementation\nmodel = Model(inputs=inputs, outputs=outputs, name=\"DenseNet121_SE_RANB_DualPool\")\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\n# Model summary\nprint(f\"Model created with SE blocks integrated into DenseNet121\")\nprint(f\"SE blocks added: 2 after backbone + 1 in original RANB\")\n# model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T16:45:17.417924Z","iopub.execute_input":"2025-08-17T16:45:17.418339Z","iopub.status.idle":"2025-08-17T16:45:37.417430Z","shell.execute_reply.started":"2025-08-17T16:45:17.418307Z","shell.execute_reply":"2025-08-17T16:45:37.416630Z"}},"outputs":[{"name":"stderr","text":"2025-08-17 16:45:19.071283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755449119.321838      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755449119.386253      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1755449132.468987      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nModel created with SE blocks integrated into DenseNet121\nSE blocks added: 2 after backbone + 1 in original RANB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"\n## 3. Model Training\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport numpy as np\nimport cv2\n\nbatch_size = 32  # Choose a batch size based on memory constraints\n\n# Callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n\n# ModelCheckpoint callback to save the best model\nmodel_checkpoint = ModelCheckpoint(\n    filepath='/kaggle/working/bestmodel_resnet.keras',  # Update path for ResNet model\n    monitor='val_loss',\n    save_best_only=True,\n    mode='min',\n    verbose=1\n)\n\n# Modified generator with shuffling between epochs and CLAHE preprocessing\ndef infinite_image_generator(image_paths, labels, batch_size=32, shuffle=True):\n    while True:  # Infinite loop\n        # Shuffle data at the start of each epoch\n        if shuffle:\n            indices = np.arange(len(image_paths))\n            np.random.shuffle(indices)\n            image_paths = [image_paths[i] for i in indices]\n            labels = [labels[i] for i in indices]\n        \n        # Yield batches for the current epoch\n        for i in range(0, len(image_paths), batch_size):\n            batch_paths = image_paths[i:i + batch_size]\n            batch_labels = labels[i:i + batch_size]\n            \n            batch_images = []\n            for path in batch_paths:\n                img = cv2.imread(path)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                img = cv2.resize(img, (299, 299))  # Resize to 299x299 for ResNet input\n\n                # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n                lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n                l, a, b = cv2.split(lab)  # Split into L, A, B channels\n                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Initialize CLAHE\n                cl = clahe.apply(l)  # Apply CLAHE to the L channel\n                limg = cv2.merge((cl, a, b))  # Merge back the channels\n                img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n\n                # Normalize the image to [0, 1]\n                batch_images.append(img / 299.0)\n            \n            yield np.array(batch_images), np.array(batch_labels)\n\n\n# Create generators\ntrain_gen = infinite_image_generator(X_train, y_train, batch_size, shuffle=True)\nval_gen = infinite_image_generator(X_val, y_val, batch_size, shuffle=False)\n\n# Calculate steps (round up to include all samples)\nsteps_per_epoch = (len(X_train) + batch_size - 1) // batch_size\nvalidation_steps = (len(X_val) + batch_size - 1) // batch_size\n\n# Ensure that the model is built and compiled before calling fit\nif not hasattr(model, 'built'):\n    model.build(input_shape=(None, 299, 299, 3))\n    model.built = True  # Flag to prevent re-building the model\n\n# Train the model with all callbacks\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    epochs=50,  # Adjust epochs based on your training needs\n    validation_data=val_gen,\n    validation_steps=validation_steps,\n    callbacks=[early_stop, reduce_lr, model_checkpoint]\n)\n\nprint(\"Training completed. Best model saved.\")\n\n# After training, get the best epoch (where val_loss was minimum)\nbest_epoch = np.argmin(history.history['val_loss'])\n\n# Extract metrics for the best epoch\nbest_train_loss = history.history['loss'][best_epoch]\nbest_train_acc = history.history['accuracy'][best_epoch]\nbest_val_loss = history.history['val_loss'][best_epoch]\nbest_val_acc = history.history['val_accuracy'][best_epoch]\n\nprint(f\"\\nBest Model Metrics (Epoch {best_epoch + 1}):\")\nprint(f\"Training Loss: {best_train_loss:.4f}, Training Accuracy: {best_train_acc:.4f}\")\nprint(f\"Validation Loss: {best_val_loss:.4f}, Validation Accuracy: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T16:45:43.083670Z","iopub.execute_input":"2025-08-17T16:45:43.084247Z","iopub.status.idle":"2025-08-17T19:04:41.750878Z","shell.execute_reply.started":"2025-08-17T16:45:43.084217Z","shell.execute_reply":"2025-08-17T19:04:41.750206Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1755449237.147871      89 service.cc:148] XLA service 0x7ca7d0025910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1755449237.149054      89 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1755449246.066843      89 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1755449339.025508      89 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707ms/step - accuracy: 0.6522 - loss: 0.9975\nEpoch 1: val_loss improved from inf to 1.11854, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 964ms/step - accuracy: 0.6526 - loss: 0.9964 - val_accuracy: 0.5789 - val_loss: 1.1185 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 0.9265 - loss: 0.3017\nEpoch 2: val_loss improved from 1.11854 to 0.22728, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 603ms/step - accuracy: 0.9265 - loss: 0.3016 - val_accuracy: 0.9492 - val_loss: 0.2273 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - accuracy: 0.9746 - loss: 0.1766\nEpoch 3: val_loss did not improve from 0.22728\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 609ms/step - accuracy: 0.9746 - loss: 0.1766 - val_accuracy: 0.9614 - val_loss: 0.2379 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 0.9816 - loss: 0.1578\nEpoch 4: val_loss improved from 0.22728 to 0.22472, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 602ms/step - accuracy: 0.9816 - loss: 0.1578 - val_accuracy: 0.9551 - val_loss: 0.2247 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - accuracy: 0.9781 - loss: 0.1593\nEpoch 5: val_loss improved from 0.22472 to 0.20689, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 592ms/step - accuracy: 0.9781 - loss: 0.1593 - val_accuracy: 0.9585 - val_loss: 0.2069 - learning_rate: 1.0000e-04\nEpoch 6/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step - accuracy: 0.9854 - loss: 0.1324\nEpoch 6: val_loss did not improve from 0.20689\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 549ms/step - accuracy: 0.9854 - loss: 0.1324 - val_accuracy: 0.9505 - val_loss: 0.2845 - learning_rate: 1.0000e-04\nEpoch 7/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - accuracy: 0.9754 - loss: 0.1622\nEpoch 7: val_loss improved from 0.20689 to 0.18987, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 491ms/step - accuracy: 0.9755 - loss: 0.1621 - val_accuracy: 0.9732 - val_loss: 0.1899 - learning_rate: 1.0000e-04\nEpoch 8/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.9927 - loss: 0.1101\nEpoch 8: val_loss did not improve from 0.18987\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 518ms/step - accuracy: 0.9927 - loss: 0.1102 - val_accuracy: 0.9518 - val_loss: 0.2359 - learning_rate: 1.0000e-04\nEpoch 9/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524ms/step - accuracy: 0.9894 - loss: 0.1156\nEpoch 9: val_loss did not improve from 0.18987\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 550ms/step - accuracy: 0.9894 - loss: 0.1156 - val_accuracy: 0.9744 - val_loss: 0.1920 - learning_rate: 1.0000e-04\nEpoch 10/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.9938 - loss: 0.1030\nEpoch 10: val_loss did not improve from 0.18987\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 516ms/step - accuracy: 0.9938 - loss: 0.1030 - val_accuracy: 0.9451 - val_loss: 0.3216 - learning_rate: 1.0000e-04\nEpoch 11/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - accuracy: 0.9863 - loss: 0.1312\nEpoch 11: val_loss improved from 0.18987 to 0.16752, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 531ms/step - accuracy: 0.9863 - loss: 0.1311 - val_accuracy: 0.9765 - val_loss: 0.1675 - learning_rate: 1.0000e-04\nEpoch 12/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - accuracy: 0.9924 - loss: 0.1030\nEpoch 12: val_loss improved from 0.16752 to 0.16558, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 525ms/step - accuracy: 0.9924 - loss: 0.1030 - val_accuracy: 0.9732 - val_loss: 0.1656 - learning_rate: 1.0000e-04\nEpoch 13/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - accuracy: 0.9917 - loss: 0.1035\nEpoch 13: val_loss improved from 0.16558 to 0.14506, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 521ms/step - accuracy: 0.9917 - loss: 0.1035 - val_accuracy: 0.9769 - val_loss: 0.1451 - learning_rate: 1.0000e-04\nEpoch 14/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - accuracy: 0.9867 - loss: 0.1160\nEpoch 14: val_loss did not improve from 0.14506\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 513ms/step - accuracy: 0.9867 - loss: 0.1160 - val_accuracy: 0.9706 - val_loss: 0.1756 - learning_rate: 1.0000e-04\nEpoch 15/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.9905 - loss: 0.1072\nEpoch 15: val_loss did not improve from 0.14506\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 517ms/step - accuracy: 0.9905 - loss: 0.1072 - val_accuracy: 0.9744 - val_loss: 0.1659 - learning_rate: 1.0000e-04\nEpoch 16/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.9938 - loss: 0.0956\nEpoch 16: val_loss did not improve from 0.14506\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 512ms/step - accuracy: 0.9938 - loss: 0.0956 - val_accuracy: 0.9748 - val_loss: 0.1668 - learning_rate: 1.0000e-04\nEpoch 17/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.9972 - loss: 0.0815\nEpoch 17: val_loss did not improve from 0.14506\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 520ms/step - accuracy: 0.9972 - loss: 0.0815 - val_accuracy: 0.9128 - val_loss: 0.4279 - learning_rate: 1.0000e-04\nEpoch 18/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.9931 - loss: 0.0945\nEpoch 18: val_loss improved from 0.14506 to 0.13829, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 530ms/step - accuracy: 0.9932 - loss: 0.0945 - val_accuracy: 0.9832 - val_loss: 0.1383 - learning_rate: 1.0000e-04\nEpoch 19/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.9902 - loss: 0.1063\nEpoch 19: val_loss improved from 0.13829 to 0.13168, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 528ms/step - accuracy: 0.9902 - loss: 0.1063 - val_accuracy: 0.9820 - val_loss: 0.1317 - learning_rate: 1.0000e-04\nEpoch 20/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - accuracy: 0.9932 - loss: 0.0929\nEpoch 20: val_loss did not improve from 0.13168\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 522ms/step - accuracy: 0.9932 - loss: 0.0929 - val_accuracy: 0.9740 - val_loss: 0.1589 - learning_rate: 1.0000e-04\nEpoch 21/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512ms/step - accuracy: 0.9918 - loss: 0.0975\nEpoch 21: val_loss improved from 0.13168 to 0.11040, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 548ms/step - accuracy: 0.9918 - loss: 0.0975 - val_accuracy: 0.9887 - val_loss: 0.1104 - learning_rate: 1.0000e-04\nEpoch 22/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - accuracy: 0.9976 - loss: 0.0777\nEpoch 22: val_loss improved from 0.11040 to 0.10549, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 546ms/step - accuracy: 0.9976 - loss: 0.0777 - val_accuracy: 0.9899 - val_loss: 0.1055 - learning_rate: 1.0000e-04\nEpoch 23/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531ms/step - accuracy: 0.9963 - loss: 0.0784\nEpoch 23: val_loss did not improve from 0.10549\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 556ms/step - accuracy: 0.9963 - loss: 0.0784 - val_accuracy: 0.9845 - val_loss: 0.1315 - learning_rate: 1.0000e-04\nEpoch 24/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535ms/step - accuracy: 0.9939 - loss: 0.0845\nEpoch 24: val_loss did not improve from 0.10549\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 561ms/step - accuracy: 0.9939 - loss: 0.0845 - val_accuracy: 0.9555 - val_loss: 0.2235 - learning_rate: 1.0000e-04\nEpoch 25/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - accuracy: 0.9889 - loss: 0.1044\nEpoch 25: val_loss did not improve from 0.10549\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 553ms/step - accuracy: 0.9889 - loss: 0.1044 - val_accuracy: 0.9786 - val_loss: 0.1405 - learning_rate: 1.0000e-04\nEpoch 26/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521ms/step - accuracy: 0.9981 - loss: 0.0717\nEpoch 26: val_loss did not improve from 0.10549\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 546ms/step - accuracy: 0.9981 - loss: 0.0717 - val_accuracy: 0.9786 - val_loss: 0.1629 - learning_rate: 1.0000e-04\nEpoch 27/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.9953 - loss: 0.0816\nEpoch 27: val_loss improved from 0.10549 to 0.09582, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 543ms/step - accuracy: 0.9953 - loss: 0.0816 - val_accuracy: 0.9908 - val_loss: 0.0958 - learning_rate: 1.0000e-04\nEpoch 28/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502ms/step - accuracy: 0.9961 - loss: 0.0747\nEpoch 28: val_loss did not improve from 0.09582\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 528ms/step - accuracy: 0.9961 - loss: 0.0747 - val_accuracy: 0.9849 - val_loss: 0.1067 - learning_rate: 1.0000e-04\nEpoch 29/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515ms/step - accuracy: 0.9951 - loss: 0.0795\nEpoch 29: val_loss did not improve from 0.09582\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 540ms/step - accuracy: 0.9951 - loss: 0.0795 - val_accuracy: 0.9862 - val_loss: 0.1119 - learning_rate: 1.0000e-04\nEpoch 30/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.9965 - loss: 0.0714\nEpoch 30: val_loss did not improve from 0.09582\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 546ms/step - accuracy: 0.9965 - loss: 0.0714 - val_accuracy: 0.8591 - val_loss: 0.6418 - learning_rate: 1.0000e-04\nEpoch 31/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511ms/step - accuracy: 0.9927 - loss: 0.0829\nEpoch 31: val_loss did not improve from 0.09582\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 536ms/step - accuracy: 0.9927 - loss: 0.0829 - val_accuracy: 0.9904 - val_loss: 0.1101 - learning_rate: 1.0000e-04\nEpoch 32/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505ms/step - accuracy: 0.9955 - loss: 0.0760\nEpoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 32: val_loss did not improve from 0.09582\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 530ms/step - accuracy: 0.9955 - loss: 0.0760 - val_accuracy: 0.9807 - val_loss: 0.1128 - learning_rate: 1.0000e-04\nEpoch 33/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506ms/step - accuracy: 0.9967 - loss: 0.0649\nEpoch 33: val_loss improved from 0.09582 to 0.07564, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 541ms/step - accuracy: 0.9967 - loss: 0.0649 - val_accuracy: 0.9950 - val_loss: 0.0756 - learning_rate: 5.0000e-05\nEpoch 34/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.9998 - loss: 0.0596\nEpoch 34: val_loss improved from 0.07564 to 0.06905, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 535ms/step - accuracy: 0.9998 - loss: 0.0596 - val_accuracy: 0.9975 - val_loss: 0.0690 - learning_rate: 5.0000e-05\nEpoch 35/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.9995 - loss: 0.0587\nEpoch 35: val_loss did not improve from 0.06905\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 519ms/step - accuracy: 0.9995 - loss: 0.0587 - val_accuracy: 0.9962 - val_loss: 0.0742 - learning_rate: 5.0000e-05\nEpoch 36/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503ms/step - accuracy: 0.9993 - loss: 0.0613\nEpoch 36: val_loss improved from 0.06905 to 0.06836, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 540ms/step - accuracy: 0.9993 - loss: 0.0613 - val_accuracy: 0.9954 - val_loss: 0.0684 - learning_rate: 5.0000e-05\nEpoch 37/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 1.0000 - loss: 0.0564\nEpoch 37: val_loss improved from 0.06836 to 0.06511, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 530ms/step - accuracy: 1.0000 - loss: 0.0564 - val_accuracy: 0.9966 - val_loss: 0.0651 - learning_rate: 5.0000e-05\nEpoch 38/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - accuracy: 0.9998 - loss: 0.0562\nEpoch 38: val_loss did not improve from 0.06511\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 522ms/step - accuracy: 0.9998 - loss: 0.0562 - val_accuracy: 0.9950 - val_loss: 0.0726 - learning_rate: 5.0000e-05\nEpoch 39/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506ms/step - accuracy: 0.9993 - loss: 0.0570\nEpoch 39: val_loss did not improve from 0.06511\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 532ms/step - accuracy: 0.9993 - loss: 0.0570 - val_accuracy: 0.9966 - val_loss: 0.0663 - learning_rate: 5.0000e-05\nEpoch 40/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - accuracy: 0.9988 - loss: 0.0555\nEpoch 40: val_loss improved from 0.06511 to 0.06370, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 550ms/step - accuracy: 0.9988 - loss: 0.0555 - val_accuracy: 0.9975 - val_loss: 0.0637 - learning_rate: 5.0000e-05\nEpoch 41/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.9981 - loss: 0.0569\nEpoch 41: val_loss improved from 0.06370 to 0.06048, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 543ms/step - accuracy: 0.9981 - loss: 0.0568 - val_accuracy: 0.9966 - val_loss: 0.0605 - learning_rate: 5.0000e-05\nEpoch 42/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.9996 - loss: 0.0524\nEpoch 42: val_loss did not improve from 0.06048\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 532ms/step - accuracy: 0.9996 - loss: 0.0524 - val_accuracy: 0.9912 - val_loss: 0.0901 - learning_rate: 5.0000e-05\nEpoch 43/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518ms/step - accuracy: 0.9987 - loss: 0.0551\nEpoch 43: val_loss did not improve from 0.06048\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 543ms/step - accuracy: 0.9987 - loss: 0.0551 - val_accuracy: 0.9799 - val_loss: 0.1338 - learning_rate: 5.0000e-05\nEpoch 44/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518ms/step - accuracy: 0.9973 - loss: 0.0578\nEpoch 44: val_loss did not improve from 0.06048\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 543ms/step - accuracy: 0.9973 - loss: 0.0578 - val_accuracy: 0.9937 - val_loss: 0.0747 - learning_rate: 5.0000e-05\nEpoch 45/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516ms/step - accuracy: 0.9998 - loss: 0.0491\nEpoch 45: val_loss improved from 0.06048 to 0.05978, saving model to /kaggle/working/bestmodel_resnet.keras\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 551ms/step - accuracy: 0.9998 - loss: 0.0491 - val_accuracy: 0.9975 - val_loss: 0.0598 - learning_rate: 5.0000e-05\nEpoch 46/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - accuracy: 1.0000 - loss: 0.0481\nEpoch 46: val_loss did not improve from 0.05978\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 522ms/step - accuracy: 1.0000 - loss: 0.0481 - val_accuracy: 0.9971 - val_loss: 0.0621 - learning_rate: 5.0000e-05\nEpoch 47/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503ms/step - accuracy: 1.0000 - loss: 0.0467\nEpoch 47: val_loss did not improve from 0.05978\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 528ms/step - accuracy: 1.0000 - loss: 0.0467 - val_accuracy: 0.9975 - val_loss: 0.0615 - learning_rate: 5.0000e-05\nEpoch 48/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.9991 - loss: 0.0494\nEpoch 48: val_loss did not improve from 0.05978\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 519ms/step - accuracy: 0.9991 - loss: 0.0494 - val_accuracy: 0.9815 - val_loss: 0.1163 - learning_rate: 5.0000e-05\nEpoch 49/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - accuracy: 0.9983 - loss: 0.0492\nEpoch 49: val_loss did not improve from 0.05978\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 535ms/step - accuracy: 0.9983 - loss: 0.0492 - val_accuracy: 0.9941 - val_loss: 0.0614 - learning_rate: 5.0000e-05\nEpoch 50/50\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - accuracy: 0.9997 - loss: 0.0442\nEpoch 50: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 50: val_loss did not improve from 0.05978\n\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 524ms/step - accuracy: 0.9997 - loss: 0.0442 - val_accuracy: 0.9899 - val_loss: 0.0824 - learning_rate: 5.0000e-05\nTraining completed. Best model saved.\n\nBest Model Metrics (Epoch 45):\nTraining Loss: 0.0489, Training Accuracy: 0.9998\nValidation Loss: 0.0598, Validation Accuracy: 0.9975\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# After training, get the best epoch (where val_loss was minimum)\nbest_epoch = np.argmin(history.history['val_loss'])\n\n# Extract metrics for the best epoch\nbest_train_loss = history.history['loss'][best_epoch]\nbest_train_acc = history.history['accuracy'][best_epoch]  # Use 'accuracy' or 'acc' based on your model\nbest_val_loss = history.history['val_loss'][best_epoch]\nbest_val_acc = history.history['val_accuracy'][best_epoch]  # Use 'val_accuracy' or 'val_acc'\n\nprint(f\"\\nBest Model Metrics (Epoch {best_epoch + 1}):\")\nprint(f\"Training Loss: {best_train_loss:.4f}, Training Accuracy: {best_train_acc:.4f}\")\nprint(f\"Validation Loss: {best_val_loss:.4f}, Validation Accuracy: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T19:04:41.751965Z","iopub.execute_input":"2025-08-17T19:04:41.752257Z","iopub.status.idle":"2025-08-17T19:04:41.758070Z","shell.execute_reply.started":"2025-08-17T19:04:41.752234Z","shell.execute_reply":"2025-08-17T19:04:41.757219Z"}},"outputs":[{"name":"stdout","text":"\nBest Model Metrics (Epoch 45):\nTraining Loss: 0.0489, Training Accuracy: 0.9998\nValidation Loss: 0.0598, Validation Accuracy: 0.9975\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"During training, we expect to see the training accuracy improving and validation accuracy reaching around 90%+ if all goes well (the original model achieved ~94% on test). The callbacks will ensure we don’t overfit excessively – training will likely stop when val_loss stops improving. The learning rate will reduce when needed to fine-tune towards a minimum. (Training logs would be displayed here, showing epoch-by-epoch loss and accuracy for training and validation.)\n6. Evaluation on Test Set","metadata":{}},{"cell_type":"markdown","source":"\n## 4. Evaluation on Test Set ( Loss, Accuracy, Classification Report, Confusion Matrix, ROC Curve)  \nAfter training, we evaluate the model on the held-out **test set** (10% of the data). We will calculate the overall accuracy as well as more detailed metrics: **precision, recall, and F1-score** for each class, a **confusion matrix**, and the **ROC-AUC** for each class. These metrics help understand model performance beyond accuracy – for example, whether it is equally good on all classes or if it struggles on certain ROP stages.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom collections import Counter\n\n# Define dataset directory and class names\nDATA_DIR = \"/kaggle/input/rop-1-dataset\"  # placeholder path\nclasses = [\"Normal\", \"Stage1\", \"Stage2\", \"Stage3\"]\nclass_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n\n# Collect filepaths and labels\nimage_paths = []\nlabels = []\nfor cls in classes:\n    cls_dir = os.path.join(DATA_DIR, cls)\n    for filename in os.listdir(cls_dir):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # image file\n            image_paths.append(os.path.join(cls_dir, filename))\n            labels.append(class_to_idx[cls])\n\nprint(f\"Found {len(image_paths)} images in total across {len(classes)} classes.\")\n# Count per class\ncounts = Counter(labels)\nfor cls_idx, count in counts.items():\n    print(f\"Class {classes[cls_idx]}: {count} images\")\n\n# Batch generator to load and preprocess images\ndef image_generator(image_paths, labels, batch_size=32):\n    batch_images = []\n    batch_labels = []\n    for i, path in enumerate(image_paths):\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (299, 299))\n\n        # Apply CLAHE\n        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n\n        batch_images.append(img / 299.0)  # normalize\n        batch_labels.append(labels[i])\n\n        if len(batch_images) == batch_size:\n            yield np.array(batch_images), np.array(batch_labels)\n            batch_images, batch_labels = [], []\n\n    if batch_images:  # last incomplete batch\n        yield np.array(batch_images), np.array(batch_labels)\n\n# All data is test data\nX_test = image_paths\ny_test = np.array(labels, dtype=np.int32)\n\n# Class distribution in test set\ndef print_class_dist(y_arr, name):\n    counts = Counter(y_arr)\n    dist = {classes[c]: counts[c] for c in sorted(counts.keys())}\n    print(f\"{name} class distribution:\", dist)\n\nprint_class_dist(y_test, \"Test\")\n\n# Create test generator\ntest_gen = image_generator(X_test, y_test, batch_size=32)\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(test_gen, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T19:04:41.758704Z","iopub.execute_input":"2025-08-17T19:04:41.758881Z","iopub.status.idle":"2025-08-17T19:05:50.206764Z","shell.execute_reply.started":"2025-08-17T19:04:41.758867Z","shell.execute_reply":"2025-08-17T19:05:50.205993Z"}},"outputs":[{"name":"stdout","text":"Found 4000 images in total across 4 classes.\nClass Normal: 1000 images\nClass Stage1: 1000 images\nClass Stage2: 1000 images\nClass Stage3: 1000 images\nTest class distribution: {'Normal': 1000, 'Stage1': 1000, 'Stage2': 1000, 'Stage3': 1000}\nTest Loss: 4.5326, Test Accuracy: 0.3830\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Confusion Matrix and Classification Report Calculation\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndef finite_image_generator(image_paths, labels, batch_size=32, shuffle=False):\n    \"\"\"Generator that yields batches until all data is exhausted with CLAHE preprocessing\"\"\"\n    num_samples = len(image_paths)\n    indices = np.arange(num_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        batch_images = []\n        batch_labels = []\n        for i in batch_indices:\n            img = cv2.imread(image_paths[i])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (299, 299))  # Resize to 299x299\n\n            # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n            l, a, b = cv2.split(lab)  # Split into L, A, B channels\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Initialize CLAHE\n            cl = clahe.apply(l)  # Apply CLAHE to L channel\n            limg = cv2.merge((cl, a, b))  # Merge back the channels\n            img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n\n            # Normalize the image to [0, 1]\n            batch_images.append(img / 299.0)\n            batch_labels.append(labels[i])\n        \n        yield np.array(batch_images), np.array(batch_labels)\n\n# Create finite generator\ntest_gen = finite_image_generator(X_test, y_test, batch_size=32, shuffle=False)\n\nall_preds = []\nall_true = []\n\ntry:\n    for x_batch, y_batch in test_gen:\n        y_pred_prob = model.predict(x_batch, verbose=0)\n        y_pred = np.argmax(y_pred_prob, axis=1)\n        all_preds.extend(y_pred)\n        all_true.extend(y_batch)\nexcept StopIteration:\n    pass  # Generator is exhausted\n\n# Metrics\nprint(\"Classification Report:\")\nprint(classification_report(all_true, all_preds, target_names=classes))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(all_true, all_preds)\nprint(cm)\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=classes, yticklabels=classes)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('True Class')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T19:05:50.208015Z","iopub.execute_input":"2025-08-17T19:05:50.208243Z","iopub.status.idle":"2025-08-17T19:07:06.665914Z","shell.execute_reply.started":"2025-08-17T19:05:50.208226Z","shell.execute_reply":"2025-08-17T19:07:06.665217Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.84      0.34      0.49      1000\n      Stage1       0.44      0.16      0.23      1000\n      Stage2       0.31      0.76      0.44      1000\n      Stage3       0.36      0.27      0.31      1000\n\n    accuracy                           0.38      4000\n   macro avg       0.49      0.38      0.37      4000\nweighted avg       0.49      0.38      0.37      4000\n\n\nConfusion Matrix:\n[[345 106 394 155]\n [ 31 158 656 155]\n [ 15  56 756 173]\n [ 18  43 666 273]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"#ROC\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\ndef safe_image_generator(image_paths, labels, batch_size=32, shuffle=False):\n    \"\"\"Generator that safely yields batches without infinite loop and includes CLAHE preprocessing\"\"\"\n    num_samples = len(image_paths)\n    indices = np.arange(num_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        batch_images = []\n        batch_labels = []\n        for i in batch_indices:\n            img = cv2.imread(image_paths[i])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (299, 299))  # Resize to 299x299\n\n            # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n            l, a, b = cv2.split(lab)  # Split into L, A, B channels\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Initialize CLAHE\n            cl = clahe.apply(l)  # Apply CLAHE to L channel\n            limg = cv2.merge((cl, a, b))  # Merge back the channels\n            img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n\n            # Normalize the image to [0, 1]\n            batch_images.append(img / 299.0)\n            batch_labels.append(labels[i])\n        \n        yield np.array(batch_images), np.array(batch_labels)\n\n# Initialize storage\nall_true = []\nall_pred_probs = []\n\n# Create safe generator\ntest_gen = safe_image_generator(X_test, y_test, batch_size=32, shuffle=False)\n\n# Process all test data\nfor x_batch, y_batch in test_gen:\n    batch_pred_probs = model.predict(x_batch, verbose=0)\n    all_true.extend(y_batch)\n    all_pred_probs.extend(batch_pred_probs)\n\n# Convert to numpy arrays and trim\ny_true = np.array(all_true)[:len(X_test)]\ny_pred_prob = np.array(all_pred_probs)[:len(X_test)]\n\n# Binarize the labels for multiclass ROC\ny_true_bin = label_binarize(y_true, classes=np.arange(len(classes)))\n\n# Compute ROC metrics for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes)):\n    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot ROC curves\nplt.figure(figsize=(10, 8))\ncolors = cycle(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n\nfor i, color in zip(range(len(classes)), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2.5,\n             label='{0} (AUC = {1:0.3f})'\n             ''.format(classes[i], roc_auc[i]))\n\n# Plot micro-average curve\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='Micro-average (AUC = {0:0.3f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\n# Plot chance line\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('Multiclass ROC Curves', fontsize=14)\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n# Print AUC scores\nprint(\"\\nAUC Scores:\")\nfor i in range(len(classes)):\n    print(f\"{classes[i]:<15}: {roc_auc[i]:.4f}\")\nprint(f\"\\nMicro-average AUC: {roc_auc['micro']:.4f}\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T19:07:06.666709Z","iopub.execute_input":"2025-08-17T19:07:06.667189Z","iopub.status.idle":"2025-08-17T19:08:09.471790Z","shell.execute_reply.started":"2025-08-17T19:07:06.667153Z","shell.execute_reply":"2025-08-17T19:08:09.470804Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1143336256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Compute micro-average ROC curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16000, 20000]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [16000, 20000]","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"\n## 7. Explainable AI – Grad-CAM and LIME  \nTo ensure the model’s decisions are trustworthy and align with clinical reasoning, we apply Explainable AI (XAI) techniques. LightEyeNet uses Grad-CAM (Gradient-weighted Class Activation Mapping) and LIME (Local Interpretable Model-Agnostic Explanations) to interpret the model’s predictions. These help highlight which parts of the input image influenced the model’s classification. Grad-CAM: Grad-CAM produces a heatmap over the input image by using the gradients of the predicted class score with respect to the feature maps of a convolutional layer​\npmc.ncbi.nlm.nih.gov\n. We will generate Grad-CAM for some test images to see where the model is “looking.” Typically, we use the last convolutional layer of the CNN (InceptionV3) for Grad-CAM. The steps are:\nGet the feature maps of the last conv layer and the model’s output.\nCompute the gradients of the target class score with respect to those feature maps.\nAverage those gradients across the spatial dimensions to get importance weights for each feature map channel.\nCompute a weighted sum of the feature maps (followed by ReLU to keep only positive influences).\nUpsample the resulting heatmap to the original image size and overlay it on the image.\nLet's implement a Grad-CAM function for our model:","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport random\nz=random.randint(0, 1500)\n# Define dataset directory and class names\nDATA_DIR = \"/kaggle/input/modified-and-augmented-rop/Modified ROP\"  # Update with your actual path\nclasses = [\"Normal\", \"Stage 0 ROP\", \"Stage 1 ROP\", \"Stage 2 ROP\", \"Stage 3 ROP\"]\nclass_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n\n# Collect filepaths and labels\nimage_paths = []\nlabels = []\nfor cls in classes:\n    cls_dir = os.path.join(DATA_DIR, cls)\n    for filename in os.listdir(cls_dir):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # image file\n            image_paths.append(os.path.join(cls_dir, filename))\n            labels.append(class_to_idx[cls])\n\nprint(f\"Found {len(image_paths)} images in total across {len(classes)} classes.\")\n# Example: print count per class for verification\ncounts = Counter(labels)\nfor cls_idx, count in counts.items():\n    print(f\"Class {classes[cls_idx]}: {count} images\")\n\n# Batch generator to load and preprocess images on-the-fly with CLAHE\ndef image_generator(image_paths, labels, batch_size=32):\n    batch_images = []\n    batch_labels = []\n    for i, path in enumerate(image_paths):\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (299, 299))  # Resize to the input size of InceptionV3 (299x299)\n\n        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n        l, a, b = cv2.split(lab)  # Split into L, A, B channels\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Initialize CLAHE\n        cl = clahe.apply(l)  # Apply CLAHE to L channel\n        limg = cv2.merge((cl, a, b))  # Merge back the channels\n        img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n\n        batch_images.append(img / 299.0)  # Normalize to 0-1\n        batch_labels.append(labels[i])\n\n        if len(batch_images) == batch_size:\n            yield np.array(batch_images), np.array(batch_labels)\n            batch_images, batch_labels = [], []  # Reset the batch after yielding\n\n    # Yield remaining data if it's not a full batch\n    if batch_images:\n        yield np.array(batch_images), np.array(batch_labels)\n\n# Split dataset\ny = np.array(labels, dtype=np.int32)\nX_train, X_temp, y_train, y_temp = train_test_split(image_paths, y, test_size=0.2, stratify=y, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n\nprint(\"Train set:\", len(X_train), \"Validation set:\", len(X_val), \"Test set:\", len(X_test))\n\n# Verify class distribution in each split\ndef print_class_dist(y_arr, name):\n    counts = Counter(y_arr)\n    dist = {classes[c]: counts[c] for c in sorted(counts.keys())}\n    print(f\"{name} class distribution:\", dist)\n\nprint_class_dist(y_train, \"Training\")\nprint_class_dist(y_val, \"Validation\")\nprint_class_dist(y_test, \"Test\")\n\n# Select a test image (for demonstration, we take the first image in the test set)\ntest_image_path = X_test[z]\n\n# Load and preprocess the test image\ntest_image = cv2.imread(test_image_path)\ntest_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\ntest_image = cv2.resize(test_image, (299, 299))  # Resize to the input size of InceptionV3 (299x299)\ntest_image = test_image / 299.0  # Normalize the image to [0, 1]\n\n# Add batch dimension (model expects a batch of images)\ninput_image = np.expand_dims(test_image, axis=0)\n\n# Class names for the 5 categories:\nclass_names = ['Normal', 'Stage 0 ROP', 'Stage 1 ROP', 'Stage 2 ROP', 'Stage 3 ROP']\n\n# Get model prediction for the test image\npreds = model.predict(input_image)\npred_idx = np.argmax(preds[0])              # index of the predicted class\npred_label = class_names[pred_idx]          # predicted class name\n\n# (If ground truth label is known for this test image, set it here for display)\ntrue_label_idx = y_test[z]  # Assuming the first image in the test set has the true label\ntrue_label = class_names[true_label_idx]\n\nprint(f\"True label: {true_label} | Predicted label: {pred_label}\")\n\n# Identify the last convolutional layer in the model (InceptionV3 example)\nlast_conv_layer = None\nfor layer in reversed(model.layers):\n    if layer.__class__.__name__ == 'Conv2D':  # Check if the layer is Conv2D\n        last_conv_layer = layer\n        break\n\n# Create a model that outputs the feature map of the last conv layer and the final predictions\ngrad_model = tf.keras.models.Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n\n# Compute the gradient of the predicted class score with respect to the feature map of the last conv layer\nwith tf.GradientTape() as tape:\n    conv_outputs, predictions = grad_model(input_image)\n    loss = predictions[:, pred_idx]        # score for the predicted class\n    tape.watch(conv_outputs)  # Watch the convolutional output for gradients\n    grads = tape.gradient(loss, conv_outputs)\n\n# Compute the channel-wise mean of the gradients (global average pooling of gradients)\npooled_grads = tf.reduce_mean(grads[0], axis=(0, 1))\n\n# Weight the convolutional feature maps by the pooled gradients\nconv_layer_output = conv_outputs[0]        # feature map of the last conv layer for this image\nheatmap = conv_layer_output * pooled_grads  # scale each channel by corresponding gradient weight\nheatmap = tf.reduce_sum(heatmap, axis=-1)   # sum over all channels to get the heatmap\nheatmap = tf.maximum(heatmap, 0)            # apply ReLU to keep only positive influences\nheatmap /= tf.reduce_max(heatmap)           # normalize heatmap to 0–1 for visualization\nheatmap = heatmap.numpy()\n\n# Resize the heatmap to the original image size\norig_img = input_image[0]  # shape (H, W, 3)\nif orig_img.min() < 0:\n    orig_img_display = ((orig_img + 1.0) / 2.0)  # [-1,1] -> [0,1]\nelse:\n    orig_img_display = orig_img.copy()\n(H, W) = orig_img_display.shape[:2]\nheatmap_resized = cv2.resize(heatmap, (W, H))\n\n# Convert heatmap to RGB color map\nheatmap_resized = np.uint8(299 * heatmap_resized)\nheatmap_color = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n\n# Prepare the original image in uint8 format for overlay\norig_img_uint8 = np.uint8(orig_img_display * 299)\norig_img_bgr = cv2.cvtColor(orig_img_uint8, cv2.COLOR_RGB2BGR)\n\n# Overlay the heatmap on the original image\noverlay_bgr = cv2.addWeighted(orig_img_bgr, 0.6, heatmap_color, 0.4, 0)\noverlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)  # convert back to RGB for plotting\n\n# Plot the original image and the Grad-CAM heatmap overlay side by side\nplt.figure(figsize=(10, 4))\n# Original image\nplt.subplot(1, 2, 1)\nplt.imshow(orig_img_display)  # orig_img_display is in [0,1] RGB format\nplt.title(f\"Original Image\\nTrue: {true_label}, Pred: {pred_label}\")\nplt.axis('off')\n# Grad-CAM overlay\nplt.subplot(1, 2, 2)\nplt.imshow(overlay_rgb)\nplt.title(\"Grad-CAM Overlay\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:50.877878Z","iopub.execute_input":"2025-08-06T18:02:50.878169Z","iopub.status.idle":"2025-08-06T18:02:52.166399Z","shell.execute_reply.started":"2025-08-06T18:02:50.878147Z","shell.execute_reply":"2025-08-06T18:02:52.165466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lime import lime_image\nfrom skimage.segmentation import mark_boundaries\n\n# Prepare the image for LIME (revert any preprocessing to original scale 0-1)\nimage_for_lime = input_image[0].copy()  # take the image without batch dimension\nif image_for_lime.min() < 0:\n    # If the image is in [-1, 1], convert to [0, 1]\n    image_for_lime = (image_for_lime + 1.0) / 2.0\n# If the image is in [0, 255], scale it to [0, 1]\nif image_for_lime.max() > 1.0:\n    image_for_lime = image_for_lime.astype(np.float32) / 299.0\n\n# Define a prediction function for LIME that applies the same preprocessing as the model expects\ndef predict_fn(images):\n    images = np.array(images)\n    # If images come in as [0, 1] floats, convert to model's expected range\n    if images.ndim == 3:\n        images = np.expand_dims(images, axis=0)\n    # If model expects [-1,1] (InceptionV3 preprocessing), scale accordingly; otherwise use as is\n    if images.min() < 0:  # model was using InceptionV3 preprocessing\n        images = images * 2.0 - 1.0  # scale 0-1 to -1 to 1\n    # (If model expects 0-1 as trained, no change needed)\n    return model.predict(images)\n\n# Create a LimeImageExplainer and explain the prediction for the test image\nexplainer = lime_image.LimeImageExplainer()\nexplanation = explainer.explain_instance(image_for_lime, predict_fn, top_labels=1, hide_color=0, num_samples=1000)\n\n# Get the explanation for the top predicted class\nexplained_class = explanation.top_labels[0]  # this should be the same as pred_idx\n# Get image and mask highlighting the superpixels that positively contribute to that class\ntemp, mask = explanation.get_image_and_mask(explained_class, positive_only=True, num_features=5, hide_rest=False)\n# `temp` is the image with non-selected superpixels unaltered (since hide_rest=False)\n# `mask` is a binary mask where 1 indicates the superpixels contributing to the class\n\n# Overlay the mask boundaries on the original image\nlime_img = mark_boundaries(temp, mask, color=(0, 1, 0))  # green boundaries for important superpixels\n\n# Plot the LIME explanation\nplt.figure(figsize=(5, 5))\nplt.imshow(lime_img)\nplt.title(f\"LIME Explanation (Predicted: {pred_label})\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:58:29.327655Z","iopub.execute_input":"2025-08-06T17:58:29.328491Z","iopub.status.idle":"2025-08-06T17:58:49.029491Z","shell.execute_reply.started":"2025-08-06T17:58:29.328463Z","shell.execute_reply":"2025-08-06T17:58:49.028668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 8. Conclusion  \nWe have implemented the LightEyeNet model for ROP classification, following the steps from data preprocessing and augmentation to model training and explainability. The model (with an InceptionV3 backbone and dual pooling head) achieves high performance on the ROP fundus dataset, with ~94% accuracy on five classes (Normal, Stage1–3, Laser scars). Oversampling ensured each class was equally represented, improving the classifier’s balance. Using Grad-CAM and LIME, we visualized the model’s attention – reassuringly, the highlighted regions correspond to known clinical features of ROP (such as demarcation lines, ridges, and scarring). LightEyeNet’s final architecture is compact and efficient, making it feasible for deployment in real-world settings (e.g., hospitals) where computational resources may be limited. In summary, we reproduced a state-of-the-art solution for ROP stage classification that not only provides excellent accuracy but also transparency in its decision-making process, which is invaluable for building trust in AI-assisted diagnosis systems.\n","metadata":{}},{"cell_type":"markdown","source":"## References\n1. InceptionV3 Architecture – Used as the base model for feature extraction in the LightEyeNet model. The pre-trained weights from ImageNet are leveraged for the initial layers.\n\n*Source: Xia, X., Xu, C., & Nan, B. (2017). \"Inception-v3 for flower classification.\" In 2017 2nd International Conference on Image, Vision and Computing (ICIVC). IEEE.\n\n2. Grad-CAM (Gradient-weighted Class Activation Mapping) – A technique for visualizing the parts of the image that most influenced the model's predictions.\n\nSource: Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.\" International Conference on Computer Vision (ICCV).\n\n3. LIME (Local Interpretable Model-Agnostic Explanations) – A technique to explain predictions by approximating the model locally using interpretable surrogate models.\n\nSource: Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). \"Why Should I Trust You?\" Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\n\n4. ROP Dataset – The publicly available dataset for Retinopathy of Prematurity (ROP) used in the model.\n\nSource: Zhao, X., Chen, S., Zhang, S., Liu, Y., Hu, Y., Yuan, D., Xie, L., Luo, X., Zheng, M., Tian, R., Chen, Y., Tan, T., Yu, Z., Sun, Y., Wu, Z., & Zhang, G. (2024). \"A fundus image dataset for intelligent retinopathy of prematurity system.\" Scientific Data, 11(1).\n\n5. Data Augmentation and Oversampling Methods – Techniques used to balance the dataset and augment images for training, discussed in several deep learning applications for imbalanced datasets.\n\n*Source: Goodfellow, I., Bengio, Y., & Courville, A. (2016). \"Deep Learning.\" MIT Press.\n\n6. Timkovič, J., Nowaková, J., Kubíček, J. et al. Retinal Image Dataset of Infants and Retinopathy of Prematurity. Sci Data 11, 814 (2024). https://doi.org/10.1038/s41597-024-03409-7\nhttps://www.nature.com/articles/s41597-024-03409-7\n\n7. Hasal, M., Nowaková, J., Hernández-Sosa, D., Timkovič, J. (2022). Image Enhancement in Retinopathy of Prematurity. In: Barolli, L., Miwa, H. (eds) Advances in Intelligent Networking and Collaborative Systems. INCoS 2022. Lecture Notes in Networks and Systems, vol 527. Springer, Cham. https://doi.org/10.1007/978-3-031-14627-5_43\n\n8. Hasal, M., Pecha, M., Nowaková, J., Hernández-Sosa, D., Snášel, V., Timkovič, J. (2023). Retinal Vessel Segmentation by U-Net with VGG-16 Backbone on Patched Images with Smooth Blending. In: Barolli, L. (eds) Advances in Intelligent Networking and Collaborative Systems. INCoS 2023. Lecture Notes on Data Engineering and Communications Technologies, vol 182. Springer, Cham. https://doi.org/10.1007/978-3-031-40971-4_44","metadata":{}}]}